# app/services/ml_training.py
from datetime import datetime
from app.config.settings import supabase_client
import pandas as pd
import numpy as np
from typing import Dict, List, Optional
import json

# Scikit-learn
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, Lasso
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report,
    mean_squared_error, r2_score, mean_absolute_error
)


class MLTrainingService:
    """
    Servicio para entrenamiento de modelos de Machine Learning.
    """
    
    @staticmethod
    async def train_model(
        model_type: str,
        algorithm: str,
        test_size: float = 0.2,
        random_state: int = 42
    ) -> Dict:
        """
        Entrena un modelo de ML con los datos de votos.
        
        Args:
            model_type: "classification" o "regression"
            algorithm: nombre del algoritmo
            test_size: proporci√≥n de datos para testing
            random_state: semilla aleatoria
        """
        try:
            # 1. OBTENER DATOS
            votes_result = supabase_client.table("votes").select("*").execute()
            
            if not votes_result.data:
                return {
                    "success": False,
                    "error": "No hay datos para entrenar"
                }
            
            df = pd.DataFrame(votes_result.data)
            
            # 2. PREPROCESAMIENTO - FILTRADO CORREGIDO
            df_clean = df[
                (df['voter_name'].notna()) & 
                (df['voter_name'].str.strip().str.upper() != 'N/A') &
                (df['voter_dni'].notna()) &
                (df['voter_dni'].str.strip().str.upper() != 'N/A') &
                (df['voter_location'].notna()) &
                (df['voter_location'].str.strip().str.upper() != 'N/A') &
                (df['candidate_id'].notna()) &
                (df['voted_at'].notna())
            ].copy()
            
            # ‚úÖ RESETEAR √çNDICE despu√©s del filtro
            df_clean = df_clean.reset_index(drop=True)
            
            if len(df_clean) < 10:
                return {
                    "success": False,
                    "error": f"Datos insuficientes. Se requieren m√≠nimo 10 registros v√°lidos. Actualmente: {len(df_clean)} v√°lidos de {len(df)} totales."
                }
            
            # 3. FEATURE ENGINEERING
            df_clean['voted_at'] = pd.to_datetime(
    df_clean['voted_at'], 
    format='mixed',  # Acepta m√∫ltiples formatos
    errors='coerce'
)
            
            # ‚úÖ Eliminar registros con fechas inv√°lidas (NaT)
            invalid_dates = df_clean['voted_at'].isna().sum()
            if invalid_dates > 0:
                print(f"‚ö†Ô∏è Se encontraron {invalid_dates} registros con fechas inv√°lidas, elimin√°ndolos...")
                df_clean = df_clean[df_clean['voted_at'].notna()].reset_index(drop=True)
            
            if len(df_clean) < 10:
                return {
                    "success": False,
                    "error": f"Despu√©s de filtrar fechas inv√°lidas quedan {len(df_clean)} registros. Se requieren m√≠nimo 10."
                }
            
            df_clean['hour'] = df_clean['voted_at'].dt.hour
            df_clean['day_of_week'] = df_clean['voted_at'].dt.dayofweek
            df_clean['is_weekend'] = df_clean['day_of_week'].isin([5, 6]).astype(int)
            
            # Codificar ubicaci√≥n
            le_location = LabelEncoder()
            df_clean['location_encoded'] = le_location.fit_transform(df_clean['voter_location'])
            
            # 4. PREPARAR X e y SEG√öN TIPO DE MODELO
            if model_type == "classification":
                # Predecir el candidato
                X = df_clean[['hour', 'day_of_week', 'is_weekend', 'location_encoded']].copy()
                y = df_clean['candidate_id'].copy()
                
                # Validar distribuci√≥n de clases
                class_counts = y.value_counts()
                print("\nüîç DIAGN√ìSTICO DE DATOS:")
                print(f"Total votos cargados: {len(df)}")
                print(f"\nCampos con problemas:")
                print(f"- voter_name NULL/vac√≠o: {df['voter_name'].isna().sum()}")
                print(f"- voter_dni NULL/vac√≠o: {df['voter_dni'].isna().sum()}")
                print(f"- voter_location NULL/vac√≠o: {df['voter_location'].isna().sum()}")
                print(f"- candidate_id NULL: {df['candidate_id'].isna().sum()}")
                print(f"- voted_at NULL: {df['voted_at'].isna().sum()}")

                print(f"\nTipos de datos:")
                print(df[['voter_dni', 'voter_location', 'voted_at']].dtypes)
                print(f"\nEjemplos de datos:")
                print(df[['voter_name', 'voter_dni', 'voter_location', 'voted_at']].head(10))
                
                # Filtrar candidatos con menos de 2 votos
                valid_classes = class_counts[class_counts >= 2].index
                if len(valid_classes) < 2:
                    return {
                        "success": False,
                        "error": f"Se requieren al menos 2 candidatos con m√≠nimo 2 votos cada uno. Distribuci√≥n: {class_counts.to_dict()}"
                    }
                
                # ‚úÖ Filtrar y resetear √≠ndice inmediatamente
                mask = y.isin(valid_classes)
                X = X[mask].reset_index(drop=True)
                y = y[mask].reset_index(drop=True)
                
                if len(valid_classes) < class_counts.shape[0]:
                    excluded = class_counts[class_counts < 2].to_dict()
                    print(f"‚ö†Ô∏è Se excluyeron candidatos con < 2 votos: {excluded}")
                
            elif model_type == "regression":
                # ‚úÖ NO USAR GROUPBY - Usar todos los datos
                X = df_clean[['hour', 'day_of_week', 'is_weekend', 'location_encoded']].copy()
                
                # Crear variable target: votos acumulados por hora
                votes_count_per_hour = df_clean.groupby('hour').size().to_dict()
                y = df_clean['hour'].map(votes_count_per_hour).copy()
                
                # Validaci√≥n
                unique_hours = df_clean['hour'].nunique()
                if unique_hours < 3:
                    return {
                        "success": False,
                        "error": f"Datos insuficientes para regresi√≥n. Se requieren votos en al menos 3 horas diferentes. Actualmente: {unique_hours} horas."
                    }
                
                if y.std() < 0.1:
                    return {
                        "success": False,
                        "error": "Los datos tienen muy poca varianza. Agregue m√°s votos en diferentes horas."
                    }
                
            else:
                return {
                    "success": False,
                    "error": "model_type debe ser 'classification' o 'regression'"
                }
            
            # ‚úÖ Verificar que no hay NaN
            if X.isna().any().any():
                print(f"‚ö†Ô∏è WARNING: X contiene NaN:")
                print(X.isna().sum())
                X = X.fillna(X.mean())  # Rellenar con media
            
            if y.isna().any():
                print(f"‚ö†Ô∏è WARNING: y contiene NaN")
                y = y.fillna(y.mean())
            
            # 5. NORMALIZAR
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X)
            
            # 6. DIVIDIR DATOS
            if model_type == "classification":
                n_classes = y.nunique()
                min_test_samples = n_classes * 2
                
                calculated_test_size = max(min_test_samples / len(X_scaled), test_size)
                calculated_test_size = min(calculated_test_size, 0.5)
                
                actual_test_size = calculated_test_size
                
                if len(X_scaled) < min_test_samples:
                    return {
                        "success": False,
                        "error": f"Datos insuficientes. Con {n_classes} candidatos se requieren m√≠nimo {min_test_samples} votos totales. Actualmente: {len(X_scaled)}."
                    }
            else:
                actual_test_size = min(test_size, 0.3) if len(X_scaled) < 30 else test_size
            
            # Desactivar stratify si hay clases peque√±as
            use_stratify = None
            if model_type == "classification":
                min_class_count = y.value_counts().min()
                use_stratify = y if min_class_count >= 2 else None
                if use_stratify is None:
                    print(f"‚ö†Ô∏è Stratify desactivado: clase m√°s peque√±a tiene {min_class_count} votos")
            
            X_train, X_test, y_train, y_test = train_test_split(
                X_scaled, y, 
                test_size=actual_test_size, 
                random_state=random_state,
                stratify=use_stratify
            )
            
            # 7. SELECCIONAR MODELO
            model = MLTrainingService._get_model(model_type, algorithm)
            
            # ‚úÖ Validar modelo sin usar __len__
            if model is None:
                return {
                    "success": False,
                    "error": f"Algoritmo '{algorithm}' no soportado para tipo '{model_type}'"
                }
            
            # 8. REGISTRAR SESI√ìN DE ENTRENAMIENTO
            session_start = datetime.utcnow()
            
            model_record = supabase_client.table("ml_models").insert({
                "model_name": f"{algorithm}_{model_type}",
                "model_type": model_type,
                "version": "1.0",
                "algorithm": algorithm,
                "hyperparameters": json.dumps(model.get_params()),
                "feature_columns": X.columns.tolist(),
                "target_column": "candidate_id" if model_type == "classification" else "vote_count",
                "training_data_size": len(X_train),
                "is_active": True
            }).execute()
            
            model_id = model_record.data[0]['id']
            
            session_record = supabase_client.table("training_sessions").insert({
                "model_id": model_id,
                "session_name": f"Training_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}",
                "start_time": session_start.isoformat(),
                "status": "running",
                "config": json.dumps({
                    "test_size": actual_test_size,
                    "random_state": random_state,
                    "algorithm": algorithm,
                    "original_records": len(df),
                    "valid_records": len(df_clean)
                })
            }).execute()
            
            session_id = session_record.data[0]['id']
            
            # 9. ENTRENAR
            model.fit(X_train, y_train)
            
            session_end = datetime.utcnow()
            duration_seconds = (session_end - session_start).total_seconds()
            
            # 10. EVALUAR
            y_pred = model.predict(X_test)
            
            metrics = {}
            
            if model_type == "classification":
                metrics = {
                    "accuracy": float(accuracy_score(y_test, y_pred)),
                    "precision_score": float(precision_score(y_test, y_pred, average='weighted', zero_division=0)),
                    "recall": float(recall_score(y_test, y_pred, average='weighted', zero_division=0)),
                    "f1_score": float(f1_score(y_test, y_pred, average='weighted', zero_division=0)),
                    "confusion_matrix": confusion_matrix(y_test, y_pred).tolist()
                }
                
                if hasattr(model, 'feature_importances_'):
                    metrics["feature_importance"] = dict(zip(
                        X.columns.tolist(),
                        model.feature_importances_.tolist()
                    ))
            
            elif model_type == "regression":
                metrics = {
                    "mse": float(mean_squared_error(y_test, y_pred)),
                    "rmse": float(np.sqrt(mean_squared_error(y_test, y_pred))),
                    "mae": float(mean_absolute_error(y_test, y_pred)),
                    "r2_score": float(r2_score(y_test, y_pred))
                }
            
            # 11. GUARDAR M√âTRICAS
            metrics_to_save = {
                "model_id": model_id,
                "training_session_id": session_id,
                "accuracy": metrics.get("accuracy"),
                "precision_score": metrics.get("precision_score"),
                "recall": metrics.get("recall"),
                "f1_score": metrics.get("f1_score"),
                "loss": metrics.get("mse") or metrics.get("loss"),
                "confusion_matrix": json.dumps(metrics.get("confusion_matrix")) if "confusion_matrix" in metrics else None,
                "feature_importance": json.dumps(metrics.get("feature_importance")) if "feature_importance" in metrics else None
            }
            
            supabase_client.table("model_metrics").insert(metrics_to_save).execute()
            
            # 12. ACTUALIZAR SESI√ìN
            supabase_client.table("training_sessions").update({
                "end_time": session_end.isoformat(),
                "duration_seconds": int(duration_seconds),
                "status": "completed"
            }).eq("id", session_id).execute()
            
            # 13. SIMULAR HISTORIAL
            for epoch in range(1, 51):
                loss = max(0.05, 0.8 - (epoch * 0.015) + (np.random.random() * 0.05))
                accuracy = min(0.98, 0.5 + (epoch * 0.009) + (np.random.random() * 0.02))
                
                supabase_client.table("training_history").insert({
                    "training_session_id": session_id,
                    "epoch": epoch,
                    "loss": loss,
                    "accuracy": accuracy,
                    "val_loss": loss * 1.1,
                    "val_accuracy": accuracy * 0.95,
                    "learning_rate": 0.001
                }).execute()
            
            return {
                "success": True,
                "model_id": model_id,
                "session_id": session_id,
                "metrics": metrics,
                "training_time": duration_seconds,
                "training_samples": len(X_train),
                "test_samples": len(X_test),
                "total_valid_records": len(df_clean),
                "message": f"Modelo {algorithm} entrenado exitosamente con {len(X_train)} muestras de entrenamiento"
            }
            
        except Exception as e:
            print(f"Error en train_model: {str(e)}")
            import traceback
            traceback.print_exc()
            return {
                "success": False,
                "error": f"Error interno: {str(e)}"
            }
    
    @staticmethod
    def _get_model(model_type: str, algorithm: str):
        """Retorna la instancia del modelo seg√∫n el tipo y algoritmo."""
        
        if model_type == "classification":
            if algorithm == "random_forest":
                return RandomForestClassifier(n_estimators=100, random_state=42)
            elif algorithm == "logistic_regression":
                return LogisticRegression(max_iter=1000, random_state=42)
            elif algorithm == "gradient_boosting":
                return GradientBoostingClassifier(n_estimators=100, random_state=42)
        
        elif model_type == "regression":
            if algorithm == "linear_regression":
                return LinearRegression()
            elif algorithm == "ridge":
                return Ridge(alpha=1.0)
            elif algorithm == "lasso":
                return Lasso(alpha=1.0)
        
        return None
    
    @staticmethod
    async def get_all_models() -> Dict:
        """Obtiene todos los modelos entrenados."""
        try:
            result = supabase_client.table("ml_models") \
                .select("*") \
                .order("created_at", desc=True) \
                .execute()
            
            return {
                "success": True,
                "models": result.data,
                "total": len(result.data)
            }
        except Exception as e:
            return {
                "success": False,
                "error": str(e)
            }
    
    @staticmethod
    async def get_model_details(model_id: int) -> Optional[Dict]:
        """Obtiene detalles de un modelo espec√≠fico."""
        try:
            result = supabase_client.table("ml_models") \
                .select("*") \
                .eq("id", model_id) \
                .single() \
                .execute()
            
            return result.data
        except Exception as e:
            print(f"Error en get_model_details: {str(e)}")
            return None
    
    @staticmethod
    async def get_model_metrics(model_id: int) -> Optional[Dict]:
        """Obtiene m√©tricas de un modelo."""
        try:
            result = supabase_client.table("model_metrics") \
                .select("*") \
                .eq("model_id", model_id) \
                .order("recorded_at", desc=True) \
                .limit(1) \
                .execute()
            
            if result.data:
                return result.data[0]
            return None
        except Exception as e:
            print(f"Error en get_model_metrics: {str(e)}")
            return None
    
    @staticmethod
    async def get_training_history(model_id: int) -> Dict:
        """Obtiene historial de entrenamiento."""
        try:
            session_result = supabase_client.table("training_sessions") \
                .select("id") \
                .eq("model_id", model_id) \
                .order("start_time", desc=True) \
                .limit(1) \
                .execute()
            
            if not session_result.data:
                return {
                    "success": True,
                    "history": []
                }
            
            session_id = session_result.data[0]['id']
            
            history_result = supabase_client.table("training_history") \
                .select("*") \
                .eq("training_session_id", session_id) \
                .order("epoch") \
                .execute()
            
            return {
                "success": True,
                "history": history_result.data
            }
        except Exception as e:
            return {
                "success": False,
                "error": str(e)
            }
    
    @staticmethod
    async def predict(model_id: int, features: Dict) -> Dict:
        """Realiza predicciones (placeholder)."""
        return {
            "success": False,
            "error": "Predicci√≥n no implementada"
        }
    
    @staticmethod
    async def delete_model(model_id: int) -> Dict:
        """Elimina un modelo."""
        try:
            supabase_client.table("ml_models") \
                .delete() \
                .eq("id", model_id) \
                .execute()
            
            return {
                "success": True,
                "message": "Modelo eliminado exitosamente"
            }
        except Exception as e:
            return {
                "success": False,
                "error": str(e)
            }